{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation for the cleantest testing framework!","text":"<p><code>cleantest</code> is a testing framework that brings up clean environments and mini-clusters  for developers in a hurry.</p>"},{"location":"#get-started-with-cleantest","title":"Get started with cleantest","text":"<p>New user? No problem. Just head on over to the user guide  to start learning how to use cleantest. Want to learn how to do some amazing things with cleantest in the field? Head on over to our tutorials. Want to contribute a bug fix, enhancement proposal, or pull request to cleantest?  Read our contributing guidelines  and  code of conduct.</p>"},{"location":"#meet-the-developers","title":"Meet the Developers","text":"<p>These are the folks who put in the effort to make cleantest great! Feel free to reach out to the development team if your are interested in becoming a contributor to cleantest.</p> Jason C. Nucciarone (NucciTheBoss)"},{"location":"#licensing-information","title":"Licensing information","text":"<p>The cleantest source code and complimentary documentation are licensed under the Apache Software License, version 2.0. You may not use cleantest or its documentation except in compliance with the license. Unless required by applicable law or agreed to in writing,  software distributed under the the Apache Software License, version 2.0 is distributed  on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied. See the Apache Software License, version 2.0, for the specific language  governing permissions and limitations under the License.</p> <p>An electronic copy of cleantest's license can be obtained  here.</p>"},{"location":"news/news/","title":"Your hotspot for the latest and greatest news about the development of cleantest!","text":""},{"location":"news/news/#040-rc1-released-february-18th-2023","title":"0.4.0-rc1 released -- February 18th, 2023","text":"<p>What is new?</p> <p>Over the past two months, I have been hard at work adding a lot of new features for 0.4.0 in my quest to make cleantest a great testing framework. The 0.4.0-rc1 pre-release is so that folks can start getting a feel for all the features that I have been adding. Not everything I want to have has been added to cleantest yet, but I want to give folks something new to get their hands on in the meantime. Here is what is new in 0.4.0-rc1:</p> <ul> <li>Added a CI/CD pipeline for linting and functional tests via GitHub Actions. No more hip firing here!</li> <li>Fixed mutability issues when running clean tests in sequence. Now singleton state can be rest between runs.</li> <li>Added a <code>run</code> utility for executing shell commands inside test environment instances.</li> <li>Added a <code>systemd</code> utility for controlling services inside test environment instances.</li> <li>Added a <code>apt</code> utility for interfacing with the apt package manager on Ubuntu/Debian-based test environment instances.</li> <li>General quality of life refactors. Removed methods that did not need to be there.</li> <li>New module structure - I tried to further refine imports from version 0.3.0.</li> <li>PRELIMINARY MULTI-DISTRIBUTION SUPPORT!!! You can know launch Rocky, AlmaLinux instances and more, but robust support     is not fully there yet (i.e. package macros do not work yet).</li> <li>Fixed issue #21.</li> <li>Introduction of Archon and Harness classes. Archon can be used to manually direct the test environment provider     and Harness is the new name for the legacy Provider classes. Harness is what wraps around testlets when     invoking the provider decorators. Archon can be used to set up more complex cloud deployments such as     mini high-performance computing clusters.</li> <li>Enhanced documentation. There is now a News page (what you are reading currently), a reference page     (to be completed by 0.6.0), and a community page that routes to GitHub Discussions. I also cleaned up     the home page to make it more concise.</li> <li>An actual tutorial! I know I said I would not add them until 0.5.0, but I decided to share a sneak-peak ;)</li> </ul> <p>What still needs to be done for 0.4.0?</p> <p>I still need to do a bit of work before the final 0.4.0 release is ready. Here is what still needs to be done:</p> <ul> <li>Add a <code>dnf</code> utility for interfacing with the dnf package manager.</li> <li>Add a <code>pacman</code> utility for interfacing with the pacman package manager.</li> <li>Add a <code>passwd</code> utility for creating users and groups on test environment instances.</li> <li>Low-level refactor to improve LXD API socket interaction.     See issue #32.</li> <li>Add some logging output to show cleantest's progress.     See issue #4.</li> <li>Fix bug where cleantest will fail if using LXD virtual machines instead of containers.     See issue #12.</li> </ul> <p>What comes after 0.4.0?</p> <p>I have big plans for cleantest 0.5.0. The focus of the 0.5.0 release will be cloud-interoperability, test result reporting, and adding a CLI front-end to cleantest. Here is a sneak-peak of what I am planning to do:</p> <ul> <li>Add Juju as a test environment provider.</li> <li>Add a REPL for interactively running cleantest.</li> <li>Add report generation abilities to cleantest.</li> <li>Add support for \"gambols\". More on that later ;)</li> </ul> <p>As always, feel free to make a discussion post if you have any questions and I hope you continue to enjoy using cleantest!</p>"},{"location":"news/news/#cleantest-goes-to-fosdem2023-february-6th-2023","title":"cleantest goes to FOSDEM'2023! -- February 6th, 2023","text":"<p>This past weekend I gave my talk \"Developing effective testing  pipelines for HPC applications\" in the HPC, Big Data, and Data Science devroom at FOSDEM in Brussels. This was cleantest's first public appearance! In the talk, I showcased the new features I have been adding to cleantest such as the Archon (fancy term for director)  class for directing test environment providers, the Harness class for encapsulating testlets, and general methods to my madness. The talk went off without a hitch except for when I got bitten by YouTube autoplay being on.</p> <p>Overall, I got a lot of great feedback about the current state of cleantest as well as establishing connections with lots of amazing folks in the HPC industry. Hopefully I get invited again next year! Check out the recording of my talk below if you are interested to see what I talked about!</p> <p> </p>"},{"location":"reference/coming-soon/","title":"Coming soon","text":"<p>This page is currently under construction</p> <p>Hey there from NucciTheBoss! I am currently in the process of evaluating tools for generating reference documentation from Python docstrings. Unfortunately, it is not super high on my list of priorities for the time being due to me just needing to get features into cleantest, but I plan on adding some reference documentation later on. If you have any recommendation for autogen tools or want to help me put together reference documentation, do not hesitate to get in touch! I know some of you might need the nitty gritty technical documentation!</p> <p>Reference documentation will be added in cleantest 0.6.0</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/","title":"Using a mini-HPC cluster to test batch jobs","text":"<p>Testing applications intended to run on high-performance computing clusters can be a painful process,  especially when you do not have access to the resources you need. Sometimes you just want to check if  you have the right compile flag specified, if the script you wrote actually works, or if your  application can successfully use multiple cores. One way to test your HPC software is by emulation;  instead of wasting precious compute time on your bare-metal cluster, you use a mini-HPC cluster that  provides the same functionality as the bare-metal cluster on a smaller scale.</p> <p>Using cleantest, you can build yourself a mini-HPC cluster anywhere you need it, whether it be on your laptop or a continuous integration pipeline runner. In this tutorial, you will learn how to build a mini-HPC cluster and submit a test batch jobs to the cluster's resource manager/workload scheduler. Below is a diagram outlining the architecture of the mini-HPC cluster that we are going to build with cleantest.</p> <pre><code>flowchart LR\n    subgraph identity [Identity]\n        ldap(OpenLDAP)\n    end\n    subgraph filesystem [Shared Filesystem]\n        nfs(NFS)\n    end\n    subgraph cluster [Resource Management &amp; Compute Service]\n        controller(slurmctld)\n        compute0(slurmd)\n        compute1(slurmd)\n        compute2(slurmd)\n    end\n\n    identity --&gt; filesystem\n    identity --&gt; cluster\n    filesystem --&gt; cluster\n    cluster --&gt; filesystem\n    controller --&gt; compute0\n    compute0 --&gt; controller\n    controller --&gt; compute1\n    compute1 --&gt; controller\n    controller --&gt; compute2\n    compute2 --&gt; controller</code></pre>"},{"location":"tutorials/using-a-mini-hpc-cluster/#setting-up-the-cleantest-environment","title":"Setting up the cleantest environment","text":""},{"location":"tutorials/using-a-mini-hpc-cluster/#test-dependencies","title":"Test dependencies","text":"<p>This tutorial will be using the LXD test environment provider to provide the test environment instances that will compose the mini-HPC cluster. If you do not have LXD installed on your system, please visit the Installation guide for instructions on how to set up LXD.</p> <p>This tutorial will also being using the Jinja templating engine for rendering specific configuration files for the services used inside the mini-HPC cluster. You can use the <code>pip</code> package manager to install Jinja on your system:</p> <pre><code>python3 -m pip install Jinja2\n</code></pre> <p>We will also be using <code>pytest</code> to run our \"clean tests\". <code>pytest</code> can be installed using <code>pip</code> as well:</p> <pre><code>python3 -m pip install pytest\n</code></pre>"},{"location":"tutorials/using-a-mini-hpc-cluster/#required-template-files","title":"Required template files","text":"<p>The following Jinja template files are needed for rendering service configuration files. Please create a <code>templates</code> directory in your current working directory and copy the templates to the newly created directory.</p> <p>sssd.conf.tmpl</p> <p>We will be using sssd (System Security Services Daemon) to connect clients to the mini-HPC cluster's identity service. This Jinja template will be used to render the sssd.conf file that will be used by the sssd service to locate the identity service.</p> <pre><code>[sssd]\nconfig_file_version = 2\ndomains = mini-hpc.org\n\n[domain/mini-hpc.org]\nid_provider = ldap\nauth_provider = ldap\nldap_uri = ldap://{{ ldap_server_address }}\ncache_credentials = True\nldap_search_base = dc=mini-hpc,dc=org\n</code></pre> <p>slurm.conf.tmpl</p> <p>We will be using the SLURM workload manager to provide resource management, workload scheduling, and compute service in the mini-HPC cluster. This Jinja template will be used to configure SLURM after the controller and compute nodes have been created.</p> <pre><code>SlurmctldHost={{ slurmctld_name }}({{ slurmctld_address }})\nClusterName=mini-hpc\n\nAuthType=auth/munge\nFirstJobId=65536\nInactiveLimit=120\nJobCompType=jobcomp/filetxt\nJobCompLoc=/var/log/slurm/jobcomp\nProctrackType=proctrack/linuxproc\nKillWait=30\nMaxJobCount=10000\nMinJobAge=3600\nReturnToService=0\nSchedulerType=sched/backfill\nSlurmctldLogFile=/var/log/slurm/slurmctld.log\nSlurmdLogFile=/var/log/slurm/slurmd.log\nSlurmctldPort=7002\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nStateSaveLocation=/var/spool/slurm.state\nSwitchType=switch/none\nTmpFS=/tmp\nWaitTime=30\n\n# Node Configurations\nNodeName={{ slurmd_0_name }} NodeAddr={{ slurmd_0_address }} CPUs=1 RealMemory=1000 TmpDisk=10000\nNodeName={{ slurmd_1_name }} NodeAddr={{ slurmd_1_address }} CPUs=1 RealMemory=1000 TmpDisk=10000\nNodeName={{ slurmd_2_name }} NodeAddr={{ slurmd_2_address }} CPUs=1 RealMemory=1000 TmpDisk=10000\n\n# Partition Configurations\nPartitionName=all Nodes={{ slurmd_0_name }},{{ slurmd_1_name }},{{ slurmd_2_name }} MaxTime=30 MaxNodes=3 State=UP\n</code></pre>"},{"location":"tutorials/using-a-mini-hpc-cluster/#create-the-test-file","title":"Create the test file","text":"<p>Create the file test_mini_hpc.py in your current working directory; this file is where we will write our test. Once you have created test_mini_hpc.py, add the following lines to the top of the file:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Test batch job using mini-HPC cluster created with cleantest.\"\"\"\nimport json\nimport os\nimport pathlib\nfrom io import StringIO\nfrom jinja2 import Environment, FileSystemLoader\nfrom cleantest.control.hooks import StopEnvHook\nfrom cleantest.control.lxd import InstanceConfig\nfrom cleantest.data import File\nfrom cleantest.provider import lxd, LXDArchon\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\ntemplates = Environment(loader=FileSystemLoader(root / \"templates\"))\n# Where the testlet will be added.\n...\ndef test_mini_hpc() -&gt; None:\n\"\"\"Test batch job inside mini-hpc cluster.\"\"\"\narchon = LXDArchon()\narchon.config.register_hook(\nStopEnvHook(name=\"get_result\", download=[File(\"/tmp/result\", root / \"result\")])\n)\nplaceholder = archon.config.get_instance_config(\"ubuntu-jammy-amd64\").dict()\nplaceholder[\"name\"] = \"mini-hpc-sm\"\narchon.config.add_instance_config(\nInstanceConfig(\nconfig={\n\"limits.cpu\": \"1\",\n\"limits.memory\": \"8GB\",\n\"security.privileged\": \"true\",\n\"raw.apparmor\": \"mount fstype=nfs*, mount fstype=rpc_pipefs,\",\n},\n**placeholder,\n)\n)\n# Where most of the code snippets will be appended \n# for creating the mini-HPC cluster.\n...\n</code></pre> <p>These imports/variable declarations at the beginning of the test file will be used throughout the code snippets in the rest of this tutorial.</p> <p>Inside our test function, we instantiate an instance of <code>LXDArchon</code> that will be used to control the LXD test environment provider. We also define a stop environment hook that will be used to retrieve the result of our test batch job. Lastly, we create a custom test environment instance configuration. This configuration will have the necessary privileges set so that we do not eat all of your workstation's resources as well as enabling support for NFS mounts inside the test environment instances.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#configuring-the-identity-service","title":"Configuring the identity service","text":"<p>To start, we will be configuring an identity service for our mini-HPC cluster. We cannot let user <code>root</code> be the  primary user of our mini-HPC cluster; that would create a disaster. To prevent the mini-HPC cluster from going  up into flames, we are going to use slapd (Stand-alone LDAP Daemon) to provide the identity service for our cluster.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#provision-script-for-ldap-0","title":"Provision script for <code>ldap-0</code>","text":"<p>cleantest has a built-in provisioning mechanism for test environment instances. You can write custom Python scripts that use utilities provided by cleantest to set up the test environment instance how you like. Below is the provision script that we will use to provision the <code>ldap-0</code> instance in our mini-HPC cluster. Save the provision script to the file ldap_provision_script.py in your current working directory.</p> <pre><code>#!/usr/bin/env python3\n# Copyright 2023 Jason C. Nucciarone\n# See LICENSE file for licensing details.\n\"\"\"Provision LDAP server nodes.\"\"\"\nimport pathlib\nimport tempfile\nimport textwrap\nfrom cleantest.utils import apt, systemd, run\n# Define resources needed to set up LDAP.\nslapd_preseed = textwrap.dedent(\n\"\"\"\n    slapd slapd/no_configuration boolean false\n    slapd slapd/domain string mini-hpc.org\n    slapd shared/organization string mini-hpc\n    slapd slapd/password1 password test\n    slapd slapd/password2 password test\n    slapd slapd/purge_database boolean true\n    slapd slapd/move_old_database boolean true\n    \"\"\"\n).strip(\"\\n\")\ndefault_ldif = textwrap.dedent(\n\"\"\"\n    dn: ou=People,dc=mini-hpc,dc=org\n    objectClass: organizationalUnit\n    ou: People\n    dn: ou=Groups,dc=mini-hpc,dc=org\n    objectClass: organizationalUnit\n    ou: Groups\n    dn: uid=nucci,ou=People,dc=mini-hpc,dc=org\n    uid: nucci\n    objectClass: inetOrgPerson\n    objectClass: posixAccount\n    cn: nucci\n    sn: nucci\n    givenName: nucci\n    mail: nucci@example.com\n    userPassword: test\n    uidNumber: 10000\n    gidNumber: 10000\n    loginShell: /bin/bash\n    homeDirectory: /home/nucci\n    dn: cn=nucci,ou=Groups,dc=mini-hpc,dc=org\n    cn: nucci\n    objectClass: posixGroup\n    gidNumber: 10000\n    memberUid: nucci\n    dn: cn=research,ou=Groups,dc=mini-hpc,dc=org\n    cn: research\n    objectClass: posixGroup\n    gidNumber: 10100\n    memberUid: nucci\n    \"\"\"\n).strip(\"\\n\")\n# Set up slapd service.\napt.update()\napt.install(\"slapd\", \"ldap-utils\", \"debconf-utils\")\nwith tempfile.NamedTemporaryFile() as preseed, tempfile.NamedTemporaryFile() as ldif:\npathlib.Path(preseed.name).write_text(slapd_preseed)\npathlib.Path(ldif.name).write_text(default_ldif)\nresults = run(\nf\"debconf-set-selections &lt; {preseed.name}\",\n\"dpkg-reconfigure -f noninteractive slapd\",\n(\n\"ldapadd -x -D cn=admin,dc=mini-hpc,dc=org -w \"\nf\"test -f {ldif.name} -H ldap:///\"\n),\n)\nfor result in results:\nassert result.exit_code == 0\nsystemd.restart(\"slapd\")\n</code></pre> <p>The provision script works through the following steps:</p> <ol> <li>Import the <code>apt</code>, <code>systemd</code>, and <code>run</code> utilities for interfacing with the APT package manager and systemd on the      instance, and executing shell commands respectively.</li> <li>Define a preseed file that will be used by <code>debconf-set-selections</code> to configure the slapd service on the instance.</li> <li>Define a ldif (LDAP Data Interchange Format) file that will be used to create our test user and group.</li> <li>Update APT cache and install <code>slapd</code>, <code>ldap-utils</code>, and <code>debconf-utils</code>.</li> <li>Configure slapd service using <code>debconf-set-selections</code> and add test user and group to the LDAP server.</li> <li>Restart slapd so that new configurations will be set.</li> </ol>"},{"location":"tutorials/using-a-mini-hpc-cluster/#use-cleantest-archon-to-create-ldap-0","title":"Use cleantest archon to create <code>ldap-0</code>","text":"<p>Now that we have our provision script ldap_provision_script.py written, use the following block of code to add the instance <code>ldap-0</code> to our LXD test environment provider:</p> <pre><code>archon.add(\n\"ldap-0\",\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"ldap_provision_script.py\",\n)\n</code></pre> <p>The provision script will be injected into <code>ldap-0</code> and executed after the instance becomes active.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#setting-up-the-shared-filesystem","title":"Setting up the shared filesystem","text":"<p>We will be using NFS (Network File System) to provide the shared filesystem in the mini-HPC cluster. Many compute instances are generally operating on the same set of data across the cluster, so the compute nodes all need access to the same data. Using NFS, we can ensure that each compute instance has access to the same file and/or directories. </p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#provision-script-for-nfs-0","title":"Provision script for <code>nfs-0</code>","text":"<p>With the <code>ldap-0</code> instance created, create the provision script nfs_provision_script.py in your current working directory and add the following code block to the file:</p> <pre><code>#!/usr/bin/env python3\n# Copyright 2023 Jason C. Nucciarone\n# See LICENSE file for licensing details.\n\"\"\"Provision NFS server nodes.\"\"\"\nimport pathlib\nimport textwrap\nfrom cleantest.utils import apt, systemd, run\n# Define resources needed to set up nfs-kernel-server.\ndefault_exports = textwrap.dedent(\n\"\"\"\n    /srv     *(ro,sync,subtree_check)\n    /home    *(rw,sync,no_subtree_check)\n    /data    *(rw,sync,no_subtree_check,no_root_squash)\n    /opt     *(rw,sync,no_subtree_check,no_root_squash)\n    \"\"\"\n).strip(\"\\n\")\n# Set up SSSD service.\napt.update()\napt.install(\"nfs-kernel-server\", \"sssd-ldap\")\nfor result in run(\n\"mv /root/.init/sssd.conf /etc/sssd/sssd.conf\",\n\"chmod 0600 /etc/sssd/sssd.conf\",\n\"pam-auth-update --enable mkhomedir\",\n):\nassert result.exit_code == 0\nsystemd.restart(\"sssd\")\n# Set up NFS kernel server.\nfor result in run(\n\"mkdir -p /data/nucci\",\n\"mkdir -p /home/nucci\",\n\"chown -R nucci:nucci /data/nucci\",\n\"chown -R nucci:nucci /home/nucci\",\n\"chmod 0755 /data\",\n\"chmod -R 0750 /data/nucci\",\n\"chmod -R 0740 /home/nucci\",\n\"ln -s /data/nucci /home/nucci/data\",\n):\nassert result.exit_code == 0\npathlib.Path(\"/etc/exports\").write_text(default_exports)\nfor result in run(\"exportfs -a\"):\nassert result.exit_code == 0\nsystemd.restart(\"nfs-kernel-server\")\n</code></pre> Connecting to the LDAP server on <code>ldap-0</code> using sssd <p>Notice how in nfs_provision_script.py when are configuring the sssd service, we are first moving a sssd.conf file from <code>/root/.init/sssd.conf</code> to <code>/etc/sssd/sssd.conf</code>? You may be wondering \"how did that file get there?\" Worry not! In the next section, we will be generating that sssd.conf file using a Jinja template and uploading to <code>nfs-0</code> as a \"provisioning resource.\"</p> <p>This script will work through the following the steps to configure the NFS server that will provide the shared filesystem for the mini-HPC cluster:</p> <ol> <li>Import <code>apt</code>, <code>systemd</code>, and <code>run</code> utilities for interfacing with the APT package manager and systemd on the      instance, and executing shell commands respectively.</li> <li>Define the exports file that will be used to tell the NFS kernel server which directories to export.</li> <li>Update the APT cache and install <code>nfs-kernel-server</code> and <code>sssd-ldap</code> for running the NFS server and connecting to     the LDAP server respectively.</li> <li>Connect to the LDAP server on <code>ldap-0</code> using sssd.</li> <li>Set up the test user's home and data directories.</li> <li>Share the <code>/data</code> and <code>/home</code> directories across the network.</li> <li>Restart the <code>nfs-kernel-service</code> so the new /etc/exports file takes effect.</li> </ol>"},{"location":"tutorials/using-a-mini-hpc-cluster/#use-cleantest-archon-to-create-nfs-0","title":"Use cleantest archon to create <code>nfs-0</code>","text":"<p>Now with our nfs_provision_script.py file created, use the following block of code to add the <code>nfs-0</code>  instance to our LXD test environment provider.</p> <pre><code>sssd_conf = StringIO(\ntemplates.get_template(\"sssd.conf.tmpl\").render(\nldap_server_address=archon.get_public_address(\"ldap-0\")\n)\n)\narchon.add(\n\"nfs-0\",\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"nfs_provision_script.py\",\nresources=[File(sssd_conf, \"/root/.init/sssd.conf\")],\n)\n</code></pre> <p>The provision script and generated sssd.conf file will be injected into <code>nfs-0</code> after the instance becomes active.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#starting-the-slurm-cluster","title":"Starting the SLURM cluster","text":"<p>SLURM is one of the most popular open-source workload managers out there for HPC. SLURM scales very well, so we can use it inside the mini-HPC cluster even if SLURM is meant to used with multi-thousand node clusters.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#provision-script-for-slurmctld-0","title":"Provision script for <code>slurmctld-0</code>","text":"<p>Now that we have both the <code>ldap-0</code> and <code>nfs-0</code> instances created, it is time to provision to controller server <code>slurmctld-0</code> for the mini-HPC cluster. Create the file slurmctld_provision_script.py in your current working directory and copy the following code block to it:</p> <pre><code>#!/usr/bin/env python3\n# Copyright 2023 Jason C. Nucciarone\n# See LICENSE file for licensing details.\n\"\"\"Provision slurmctld nodes.\"\"\"\nimport pathlib\nimport json\nfrom io import StringIO\nfrom cleantest.utils import apt, systemd, run\n# Set up SSSD service.\napt.update()\napt.install(\"slurmctld\", \"nfs-common\", \"sssd-ldap\")\nfor result in run(\n\"mv /root/.init/sssd.conf /etc/sssd/sssd.conf\",\n\"chmod 0600 /etc/sssd/sssd.conf\",\n\"pam-auth-update --enable mkhomedir\",\n):\nassert result.exit_code == 0\nsystemd.restart(\"sssd\")\n# Set up NFS mount.\nnfs_ip = json.load(StringIO(pathlib.Path(\"/root/.init/nfs-0\").read_text()))\nfor result in run(\nf\"mount {nfs_ip['nfs-0']}:/home /home\",\n\"mkdir -p /data\",\nf\"mount {nfs_ip['nfs-0']}:/data /data\",\n):\nassert result.exit_code == 0\n</code></pre> Mounting directories shared by <code>nfs-0</code> and connecting to LDAP server on <code>ldap-0</code> <p>Notice how like when we provisioned the <code>nfs-0</code> instance, we used a \"provisioning resource\" to connect the sssd service to the LDAP server? In the above provisioning script for <code>slurmctld-0</code>, we use the same mechanism for mounting the shared directories on <code>nfs-0</code>. We will create this <code>/root/.init/nfs-0</code> resource in  the section about creating the <code>slurmctld-0</code> instance.</p> <p>This script will work through the following steps to configure the controller service for the mini-HPC cluster. We will be starting the control server manually rather than having the provision script start it for us:</p> <ol> <li>Import <code>apt</code>, <code>systemd</code>, and <code>run</code> utilities for interfacing with the APT package manager and systemd on the      instance, and executing shell commands respectively.</li> <li>Update the APT cache and install <code>slurmctld</code>, <code>nfs-common</code>, <code>sssd-ldap</code> for running the controller server,     mounting the shared directories, and connecting to the LDAP server respectively.</li> <li>Connect to the LDAP server running on <code>ldap-0</code> using sssd.</li> <li>Mount the shared directories exported by the NFS server running on <code>nfs-0</code>.</li> </ol>"},{"location":"tutorials/using-a-mini-hpc-cluster/#provision-script-for-slurmd-012","title":"Provision script for <code>slurmd-{0,1,2}</code>","text":"<p>We can also provision the compute nodes alongside the controller server <code>slurmctld-0</code>. Create the file slurmd_provision_script.py in your current working directory and copy the following code block to it:</p> <pre><code>#!/usr/bin/env python3\n# Copyright 2023 Jason C. Nucciarone\n# See LICENSE file for licensing details.\n\"\"\"Provision slurmd nodes.\"\"\"\nimport json\nimport pathlib\nfrom io import StringIO\nfrom cleantest.utils import apt, systemd, run\n# Set up SSSD service.\napt.update()\napt.install(\"slurmd\", \"nfs-common\", \"sssd-ldap\")\nfor result in run(\n\"mv /root/.init/sssd.conf /etc/sssd/sssd.conf\",\n\"chmod 0600 /etc/sssd/sssd.conf\",\n\"pam-auth-update --enable mkhomedir\",\n):\nassert result.exit_code == 0\nsystemd.restart(\"sssd\")\n# Set up NFS mount.\nnfs_ip = json.load(StringIO(pathlib.Path(\"/root/.init/nfs-0\").read_text()))\nfor result in run(\nf\"mount {nfs_ip['nfs-0']}:/home /home\",\n\"mkdir -p /data\",\nf\"mount {nfs_ip['nfs-0']}:/data /data\",\n):\nassert result.exit_code == 0\n# Set up munge key.\nfor result in run(\n\"mv /root/.init/munge.key /etc/munge/munge.key\",\n\"chown munge:munge /etc/munge/munge.key\",\n\"chmod 0600 /etc/munge/munge.key\",\n):\nassert result.exit_code == 0\nsystemd.restart(\"munge\")\n</code></pre> Setting up MUNGE authentication service <p>Similar to how <code>nfs-0</code> and <code>slurmctld-0</code> are provisioned, the slurmd instances also use \"provisioning resources.\" However, unlike those instances, the slurmd instances require a MUNGE key from the <code>slurmctld-0</code>; MUNGE is the authentication service that the SLURM workload manager uses to verify nodes in SLURM cluster.  The section on creating the slurmd instances will show you how to pull resources from other instances.</p> <p>This script will work through the following steps to configure the compute service for the mini-HPC cluster. We will be starting the compute servers manually rather than having the provision script start it for us:</p> <ol> <li>Import <code>apt</code>, <code>systemd</code>, and <code>run</code> utilities for interfacing with the APT package manager and systemd on the      instance, and executing shell commands respectively.</li> <li>Update the APT cache and install <code>slurmd</code>, <code>nfs-common</code>, <code>sssd-ldap</code> for running the compute server,     mounting the shared directories, and connecting to the LDAP server respectively.</li> <li>Connect to the LDAP server running on <code>ldap-0</code> using sssd.</li> <li>Mount the shared directories exported by the NFS server running on <code>nfs-0</code>.</li> <li>Set up MUNGE key pulled from <code>slurmctld-0</code> instance.</li> </ol>"},{"location":"tutorials/using-a-mini-hpc-cluster/#use-cleantest-archon-to-create-slurmctld-0","title":"Use cleantest archon to create <code>slurmctld-0</code>","text":"<p>Now with our slurmctld_provision_script.py file created, use the following block of code to add the <code>slurmctld-0</code>  instance to our LXD test environment provider.</p> <pre><code>nfs_ip = json.dumps({\"nfs-0\": str(archon.get_public_address(\"nfs-0\"))})\narchon.add(\n\"slurmctld-0\",\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"slurmctld_provision_script.py\",\nresources=[\nFile(sssd_conf, \"/root/.init/sssd.conf\"),\nFile(StringIO(nfs_ip), \"/root/.init/nfs-0\"),\n],\n)\n</code></pre> <p>The provision script, generated sssd.conf and generated nfs-0 files will be injected into <code>slurmctld-0</code> after the instance becomes active.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#pull-munge-key-from-slurmctld-0-and-create-slurmd-012","title":"Pull MUNGE key from <code>slurmctld-0</code> and create <code>slurmd-{0,1,2}</code>","text":"<p>Using our other provisioning script slurmd_provision_script.py, use the following code block to pull the  munge.key file from <code>slurmctld-0</code> and create the instance <code>slurmd-0</code>, <code>slurmd-1</code>, and <code>slurmd-2</code>.</p> <pre><code>archon.pull(\n\"slurmctld-0\", data_obj=[File(\"/etc/munge/munge.key\", root / \"munge.key\")]\n)\narchon.add(\n[\"slurmd-0\", \"slurmd-1\", \"slurmd-2\"],\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"slurmd_provision_script.py\",\nresources=[\nFile(sssd_conf, \"/root/.init/sssd.conf\"),\nFile(StringIO(nfs_ip), \"/root/.init/nfs-0\"),\nFile(root / \"munge.key\", \"/root/.init/munge.key\"),\n],\n)\n</code></pre> <p>The provision script, generated sssd.conf file, generated nfs-0 file, and pulled munge.key file will be  injected into <code>slurmd-0</code>, <code>slurmd-1</code>, and <code>slurmd-2</code> after the instance becomes active.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#sync-slurmconf-across-slurmctld-0-and-slurmd-012-and-start-slurm-services","title":"Sync slurm.conf across <code>slurmctld-0</code> and <code>slurmd-{0,1,2}</code> and start SLURM services","text":"<p>Now that <code>slurmctld-0</code>, <code>slurmd-0</code>, <code>slurmd-1</code>, and <code>slurmd-2</code> have been created, we can now use the following code block to start the SLURM cluster:</p> <pre><code>slurm_node_info = {\n\"slurmctld_name\": \"slurmctld-0\",\n\"slurmctld_address\": archon.get_public_address(\"slurmctld-0\"),\n\"slurmd_0_name\": \"slurmd-0\",\n\"slurmd_0_address\": archon.get_public_address(\"slurmd-0\"),\n\"slurmd_1_name\": \"slurmd-1\",\n\"slurmd_1_address\": archon.get_public_address(\"slurmd-1\"),\n\"slurmd_2_name\": \"slurmd-2\",\n\"slurmd_2_address\": archon.get_public_address(\"slurmd-2\"),\n}\nslurm_conf = StringIO(\ntemplates.get_template(\"slurm.conf.tmpl\").render(**slurm_node_info)\n)\nfor node in [\"slurmctld-0\", \"slurmd-0\", \"slurmd-1\", \"slurmd-2\"]:\narchon.push(node, data_obj=[File(slurm_conf, \"/etc/slurm/slurm.conf\")])\narchon.execute(\"slurmctld-0\", command=\"systemctl start slurmctld\")\narchon.execute(\n[\"slurmd-0\", \"slurmd-1\", \"slurmd-2\"], command=\"systemctl start slurmd\"\n)\n</code></pre> <p>This code block will pull the IPv4 addresses of <code>slurmctld-0</code>, <code>slurmd-0</code>, <code>slurmd-1</code>, and <code>slurmd-2</code>, generate the slurm.conf from a Jinja template, push slurm.conf into all the slurm instances, and then start each slurm service.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#write-a-testlet-to-submit-test-batch-job","title":"Write a testlet to submit test batch job","text":"<p>Congratulations, we finally have all the code that we need to create the mini-HPC cluster when we go to run our test. Now it is time to write the testlet. In the code block below, <code>@lxd.target(\"...\")</code> is used to target a specific test environment instance rather than creating a unique instance for the testlet:</p> <pre><code>@lxd.target(\"slurmctld-0\")\ndef run_job():\nimport os\nimport pathlib\nimport shutil\nimport textwrap\nfrom time import sleep\nfrom cleantest.utils import run\ntmp_dir = pathlib.Path(\"/tmp\")\n(tmp_dir / \"research.submit\").write_text(\ntextwrap.dedent(\n\"\"\"\n            #!/bin/bash\n            #SBATCH --job-name=research\n            #SBATCH --partition=all\n            #SBATCH --nodes=1\n            #SBATCH --ntasks-per-node=1\n            #SBATCH --cpus-per-task=1\n            #SBATCH --mem=500mb\n            #SBATCH --time=00:00:30\n            #SBATCH --error=research.err\n            #SBATCH --output=research.out\n            echo \"I love doing research!\"\n            \"\"\"\n).strip(\"\\n\")\n)\n# Set user to test cluster user nucci.\nos.setuid(10000)\nos.chdir(\"/home/nucci\")\nfor result in run(\nf\"cp {(tmp_dir / 'research.submit')} .\",\n\"sbatch research.submit\",\n):\nassert result.exit_code == 0\nsleep(60)\nshutil.copy(\"research.out\", (tmp_dir / \"result\"))\n</code></pre> A warning about using cleantest with the SLURM workload manager <p>One thing to note about the SLURM workload manager is that it does not let user <code>root</code> submit jobs to the scheduler (for obvious reasons). Unfortunately, cleantest currently only allows for testlets to be run as user <code>root</code>. To get around this limitation, we can use Python's <code>os</code> module to change to our test user, but we cannot change back to user <code>root</code> after the switch. You should have your \"power-user\" operations taken care of before switching to the test user.</p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#bringing-it-all-together","title":"Bringing it all together","text":"<p>Your completed test file test_mini_hpc.py should look like the following:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Test batch job using mini-HPC cluster created with cleantest.\"\"\"\nimport json\nimport os\nimport pathlib\nfrom io import StringIO\nfrom jinja2 import Environment, FileSystemLoader\nfrom cleantest.control.hooks import StopEnvHook\nfrom cleantest.control.lxd import InstanceConfig\nfrom cleantest.data import File\nfrom cleantest.provider import lxd, LXDArchon\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\ntemplates = Environment(loader=FileSystemLoader(root / \"templates\"))\n@lxd.target(\"slurmctld-0\")\ndef run_job():\nimport os\nimport pathlib\nimport shutil\nimport textwrap\nfrom time import sleep\nfrom cleantest.utils import run\ntmp_dir = pathlib.Path(\"/tmp\")\n(tmp_dir / \"research.submit\").write_text(\ntextwrap.dedent(\n\"\"\"\n            #!/bin/bash\n            #SBATCH --job-name=research\n            #SBATCH --partition=all\n            #SBATCH --nodes=1\n            #SBATCH --ntasks-per-node=1\n            #SBATCH --cpus-per-task=1\n            #SBATCH --mem=500mb\n            #SBATCH --time=00:00:30\n            #SBATCH --error=research.err\n            #SBATCH --output=research.out\n            echo \"I love doing research!\"\n            \"\"\"\n).strip(\"\\n\")\n)\n# Set user to test cluster user nucci.\nos.setuid(10000)\nos.chdir(\"/home/nucci\")\nfor result in run(\nf\"cp {(tmp_dir / 'research.submit')} .\",\n\"sbatch research.submit\",\n):\nassert result.exit_code == 0\nsleep(60)\nshutil.copy(\"research.out\", (tmp_dir / \"result\"))\ndef test_lxd_archon_local() -&gt; None:\n\"\"\"Test LXDArchon against local LXD cluster.\"\"\"\narchon = LXDArchon()\narchon.config.register_hook(\nStopEnvHook(name=\"get_result\", download=[File(\"/tmp/result\", root / \"result\")])\n)\nplaceholder = archon.config.get_instance_config(\"ubuntu-jammy-amd64\").dict()\nplaceholder[\"name\"] = \"mini-hpc-sm\"\narchon.config.add_instance_config(\nInstanceConfig(\nconfig={\n\"limits.cpu\": \"1\",\n\"limits.memory\": \"8GB\",\n\"security.privileged\": \"true\",\n\"raw.apparmor\": \"mount fstype=nfs*, mount fstype=rpc_pipefs,\",\n},\n**placeholder,\n)\n)\narchon.add(\n\"ldap-0\",\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"ldap_provision_script.py\",\n)\nsssd_conf = StringIO(\ntemplates.get_template(\"sssd.conf.tmpl\").render(\nldap_server_address=archon.get_public_address(\"ldap-0\")\n)\n)\narchon.add(\n\"nfs-0\",\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"nfs_provision_script.py\",\nresources=[File(sssd_conf, \"/root/.init/sssd.conf\")],\n)\nnfs_ip = json.dumps({\"nfs-0\": str(archon.get_public_address(\"nfs-0\"))})\narchon.add(\n\"slurmctld-0\",\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"slurmctld_provision_script.py\",\nresources=[\nFile(sssd_conf, \"/root/.init/sssd.conf\"),\nFile(StringIO(nfs_ip), \"/root/.init/nfs-0\"),\n],\n)\narchon.pull(\n\"slurmctld-0\", data_obj=[File(\"/etc/munge/munge.key\", root / \"munge.key\")]\n)\narchon.add(\n[\"slurmd-0\", \"slurmd-1\", \"slurmd-2\"],\nimage=\"mini-hpc-sm\",\nprovision_script=root / \"slurmd_provision_script.py\",\nresources=[\nFile(sssd_conf, \"/root/.init/sssd.conf\"),\nFile(StringIO(nfs_ip), \"/root/.init/nfs-0\"),\nFile(root / \"munge.key\", \"/root/.init/munge.key\"),\n],\n)\nslurm_node_info = {\n\"slurmctld_name\": \"slurmctld-0\",\n\"slurmctld_address\": archon.get_public_address(\"slurmctld-0\"),\n\"slurmd_0_name\": \"slurmd-0\",\n\"slurmd_0_address\": archon.get_public_address(\"slurmd-0\"),\n\"slurmd_1_name\": \"slurmd-1\",\n\"slurmd_1_address\": archon.get_public_address(\"slurmd-1\"),\n\"slurmd_2_name\": \"slurmd-2\",\n\"slurmd_2_address\": archon.get_public_address(\"slurmd-2\"),\n}\nslurm_conf = StringIO(\ntemplates.get_template(\"slurm.conf.tmpl\").render(**slurm_node_info)\n)\nfor node in [\"slurmctld-0\", \"slurmd-0\", \"slurmd-1\", \"slurmd-2\"]:\narchon.push(node, data_obj=[File(slurm_conf, \"/etc/slurm/slurm.conf\")])\narchon.execute(\"slurmctld-0\", command=\"systemctl start slurmctld\")\narchon.execute(\n[\"slurmd-0\", \"slurmd-1\", \"slurmd-2\"], command=\"systemctl start slurmd\"\n)\nfor name, result in run_job():\nassert \"I love doing research!\" in pathlib.Path(root / \"result\").read_text()\n(root / \"munge.key\").unlink(missing_ok=True)\n(root / \"result\").unlink(missing_ok=True)\narchon.execute(\n[\"slurmctld-0\", \"slurmd-0\", \"slurmd-1\", \"slurmd-2\"],\ncommand=f\"umount /home /data\",\n)\narchon.destroy()\n</code></pre> <p>Do not forget to add the bottom part here that evaluates the result of the testlet! After the evaluation, some basic cleanup is also performed so that the mini-HPC cluster does not continue to use your workstation's resources after the test has completed. Now use <code>pytest</code> to run your \"clean test\"!</p> <pre><code>pytest test_mini_hpc.py\n</code></pre> <p>Now sit back, relax, and wait for your test to complete! You can watch the following video to get an idea of what cleantest is doing \"under the hood\" on your system to run the test:</p> <p> </p>"},{"location":"tutorials/using-a-mini-hpc-cluster/#where-to-go-from-here","title":"Where to go from here","text":"<p>This was quite a long tutorial, so make sure to grant yourself a well-earned coffee break! </p> <p>If you are interested in learning more about the underpinnings of a mini-HPC cluster, check out my  intro to open-source HPC workshop that I gave at the 2022 Ubuntu  Summit in Prague. My workshop will take your through building your own mini-HPC cluster manually, and it takes you further than just setting up the infrastructure such as building containers and deploying your own software stack.</p> <p>If you want to sink your teeth further into cleantest, I suggest reading more of the tutorials or heading over onto the User Guide page to learn about what cleantest is capable of!</p>"},{"location":"user-guide/getting-started/","title":"Getting started with cleantest","text":"<p>Now that you have the cleantest framework installed on your system, let's get you introduced to writing some basic  tests. This example assumes that you have LXD installed and configured, and cleantest is installed correctly on  your system. If not, please revisit the installation documentation.</p> <p>Also, you will need to have pytest installed on your system as well.  You can install it using the following command:</p> <pre><code>pip install pytest\n</code></pre>"},{"location":"user-guide/getting-started/#some-background","title":"Some background","text":"<p>The goal of the cleantest framework is to provide an easy way to bring up clean testing environments without a  hassle. A \"clean test\" is a test written with cleantest that can drop on top of a pre-existing testing framework  such as pytest or unittest. These \"clean tests\" can be broken down into three parts: </p> <ol> <li>Configuration statements</li> <li>A collection of Testlets</li> <li>One or more Test Suites</li> </ol> <p>Configuration statements control the state and flow of cleantest, testlets are the tests to be run in the testing environment, and the test suites define the order in which the testlets are executed.</p>"},{"location":"user-guide/getting-started/#defining-a-testlet","title":"Defining a testlet","text":"<p>A testlet is essentially an entire Python script encapsulated in a function; it contains all the imports,  definitions, and logic a program needs to run. Here is an example of a testlet below:</p> <pre><code>from cleantest.provider import lxd\n@lxd()\ndef do_something():\nimport sys\ntry:\nimport urllib\nsys.exit(0)\nexcept ImportError:\nsys.exit(1)\n</code></pre> <p>An important thing to note about testlets is that they are not run in the same Python interpreter as the test suite.  The testlets are actually picked up by the interpreter and run somewhere else, and a <code>Result</code> object is returned  containing an exit code, stdout, and stderr. Therefore, you should always import the Python modules and assets you  need within the body of the testlet, not in the main test files.</p>"},{"location":"user-guide/getting-started/#writing-a-test-suite","title":"Writing a test suite","text":"<p>To evaluate the results of a testlet, you need a test suite. This part should be invoked by your testing framework  of choice. In our case, we used pytest:</p> <pre><code>class TestSuite:\ndef test_do_something(self) -&gt; None:\nfor name, result in do_something():\nassert result.exit_code == 0\n</code></pre> <p>The test suite should be focused solely on launching testlets and evaluating the results. You should never define  testlets inside a test suite. They should always be a top-level, globally accessible function.</p>"},{"location":"user-guide/getting-started/#bringing-it-all-together","title":"Bringing it all together","text":"<p>To bring it all together, combine the testlet and test suite combined into a single Python file:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"A basic test.\"\"\"\nfrom cleantest.provider import lxd\n@lxd(preserve=False)\ndef do_something():\nimport sys\ntry:\nimport urllib\nsys.exit(0)\nexcept ImportError:\nsys.exit(1)\nclass TestSuite:\ndef test_do_something(self) -&gt; None:\nfor name, result in do_something():\nassert result.exit_code == 0\n</code></pre> <p>Now use pytest to run the test:</p> <pre><code>pytest my_cleantest.py\n</code></pre> <p>You should see the following output from your test:</p> <pre><code>=========== test session starts ===========\nplatform linux -- Python 3.10.4, pytest-7.1.3, pluggy-1.0.0\nrootdir: /mnt/d/scratch\ncollected 1 item                                                                                                                                                                                                  \n\nbasic_test.py .                                                                                                                                                                                             [100%]\n\n=========== 1 passed in 8.95s ===========\n</code></pre> <p>Congrats, you have written your first clean test!</p>"},{"location":"user-guide/getting-started/#next-steps","title":"Next steps","text":"<p>Now that you have taken your first introductory steps with cleantest, you should now go through the rest of the documentation and examples to learn about all the things that you can do with cleantest! Of course, learning through trial and error also works.</p>"},{"location":"user-guide/hooks/","title":"Using hooks","text":"<p>Hooks are used to run actions at various stages of the cleantest lifecycle. They can be used to configure test environments after they have been created, upload dependencies into the test environment, or download artifacts after the test has finished. Currently, there are two supported hook types. Their usage is described below.</p>"},{"location":"user-guide/hooks/#startenvhook","title":"StartEnvHook","text":"<p><code>StartEnvHook</code>, or start environment hook, is a hook that is run after a test environment instance has been  created and initialized. It has two main usages:</p> <ol> <li>Install dependencies needed by the testlet.</li> <li>Uploading artifacts needed by the testlet to run.</li> </ol> <p>Start environment hooks accept the following arguments:</p> <ul> <li><code>name (str)</code>: Name of the hook. Must be unique.</li> <li><code>packages (List[Injectable])</code>: List of packages to install inside the test environment instance before running the     testlet.</li> <li><code>upload (List[Injectable])</code>: List of artifacts to upload from the local system to the test environment instance.</li> </ul>"},{"location":"user-guide/hooks/#example-usage","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of StartEnvHook.\"\"\"\nimport os\nimport pathlib\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook\nfrom cleantest.data.pkg import Connection, Plug, Slot, Snap\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef functional_snaps():\nimport sys\nfrom shutil import which\nif which(\"pypi-server\") is None:\nsys.exit(1)\nelif which(\"marktext\") is None:\nsys.exit(1)\nelse:\nsys.exit(0)\ndef test_snap_package(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"test_snaps\",\npackages=[\nSnap(\nsnaps=\"pypi-server\",\nconnections=[\nConnection(\nPlug(\"pypi-server\", \"removable-media\"),\nSlot(name=\"removable-media\"),\n)\n],\n),\nSnap(\nlocal_snaps=root / \"marktext.snap\",\ndangerous=True,\n),\n],\n)\nconfig.register_hook(start_hook)\nfor name, result in functional_snaps():\nassert result.exit_code == 0\n</code></pre>"},{"location":"user-guide/hooks/#stopenvhook","title":"StopEnvHook","text":"<p><code>StopEnvHook</code>, or stop environment hook, is a hook that is run after the testlet has completed  inside the test environment instance. It has one main usage:</p> <ol> <li>Downloading artifacts after the testlet has completed.</li> </ol> <p>Stop environment hooks accept the following arguments:</p> <ul> <li><code>name (str)</code>: Name of the hook. Must be unique.</li> <li><code>download (List[Injectable])</code>: List of artifacts to download from the test environment instance to the local system.</li> </ul>"},{"location":"user-guide/hooks/#example-usage_1","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of StopEnvHook.\"\"\"\nimport os\nimport pathlib\nimport shutil\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StopEnvHook\nfrom cleantest.data import Dir, File\nfrom cleantest.provider import lxd\n@lxd(image=\"jammy-amd64\", preserve=True)\ndef work_on_artifacts():\nimport os\nimport pathlib\npathlib.Path(\"/root/dump.txt\").write_text(\"Dumped like a sack of rocks\")\nos.mkdir(\"/root/dump\")\npathlib.Path(\"/root/dump/dump_1.txt\").write_text(\"Oh I have been dumped again!\")\ndef test_download() -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstop_hook = StopEnvHook(\nname=\"download_artifact\",\ndownload=[\nFile(\"/root/dump.txt\", root / \"dump.txt\", overwrite=True,),\nDir(\"/root/dump\", root / \"dump\", overwrite=True,),\n],\n)\nconfig.register_hook(stop_hook)\nfor name, result in work_on_artifacts():\nassert (root / \"dump.txt\").is_file() is True\nassert (root / \"dump\").is_dir() is True\n(root / \"dump.txt\").unlink(missing_ok=True)\nshutil.rmtree(root / \"dump\")\n</code></pre>"},{"location":"user-guide/installation/","title":"Installation","text":""},{"location":"user-guide/installation/#install-cleantest","title":"Install cleantest","text":"<p>The latest stable release of <code>cleantest</code> can be installed using pip:</p> <pre><code>pip install cleantest\n</code></pre> <p>You can also install the latest, bleeding-edge, and potentially unstable development branch from GitHub using the following commands:</p> <pre><code>git clone https://github.com/NucciTheBoss/cleantest.git\ncd cleantest\npython3 -m pip install .\n</code></pre> Warning <p>Before you can start writing tests using cleantest, you must also set up a test environment provider.</p>"},{"location":"user-guide/installation/#setting-up-testing-environment-providers","title":"Setting up testing environment providers","text":"<p>You can use the following instructions to set up a supported test environment provider of your choice to be used with  <code>cleantest</code>. Note that you do not need to install every testing environment provider listed below; you only  need to install the providers you wish to use for testing.</p> About supported test environment providers <p>LXD is currently the only supported test environment provider. For the best experience, I encourage you to use LXD on Ubuntu. You can connect to the LXD server from other machines such as Mac, Windows, and Linux* using the LXC client.  How to set up the LXC client on Mac and Windows is beyond the scope of this documentation for now. I only currently have the means to use Ubuntu (and it is my favorite distro; sorry Arch/Fedora.) </p> <p>I have plans to add more test environment provides (ssh, libvirt/kvm, vagrant, etc.) in the future, but for now I am focused on LXD.</p>"},{"location":"user-guide/installation/#lxd","title":"LXD","text":"<p>To use LXD as a test environment provider with <code>cleantest</code>, you will need to have the snap package manager and snapd service installed on your host system. Once you have  snapd and the snap package manager on your host system, use the following command to install LXD:</p> <pre><code>sudo snap install lxd\n</code></pre> <p>After LXD has finished installing on the host system, use the following command to initialize a basic LXD cluster:</p> <pre><code>lxd init --auto\n</code></pre> <p>Once the LXD cluster finishes initializing, you can now use LXD as a test environment provider with <code>cleantest</code>.</p>"},{"location":"user-guide/parallelization/","title":"Running tests in parallel","text":"<p>cleantest has the ability to run test environment instances in parallel to increase the efficiency of test runs. You can define the number of threads to use when running test environment instances in parallel as either arguments to  the test environment provider, or you can pass the number of threads to use when the test run is started using  the <code>CLEANTEST_NUM_THREADS</code> environment variable:</p> <pre><code>CLEANTEST_NUM_THREADS=$(nproc) pytest cleantest_test_suite.py\n</code></pre> <p>This is the flow that cleantest follows when parallel testing is enabled for a testlet. Rather than proceeding through each image sequentially to bring up a test environment instance and inject the testlet, each instance is brought up independently in a separate thread and the testlet is injected there. cleantest blocks until all threads have  completed. </p> <pre><code>stateDiagram-v2\n    start : @testenvprovider(name=\"test\", image=[\"jammy-amd64\", \"focal-amd64\", \"bionic-amd64\"])\n    instances : [\"test-jammy-amd64\", \"test-focal-amd64\", \"test-bionic-amd64\"]\n    jammy : test-jammy-amd64\n    focal : test-focal-amd64\n    bionic : test-bionic-amd64\n\n    [*] --&gt; start: Test run is started\n    start --&gt; instances: cleantest identifies instances to be created\n    instances --&gt; jammy: Thread #1\n    instances --&gt; focal: Thread #2\n    instances --&gt; bionic: Thread #3\n    jammy --&gt; Results: Result of testlet\n    focal --&gt; Results: Result of testlet\n    bionic --&gt; Results: Result of testlet\n    Results --&gt; Evaluation: Result from each instance is evaluated on local system\n    Evaluation --&gt; [*]</code></pre> <p>cleantest's built-in parallel testing support is best suited for spread testing; you have a testlet that you need to test across different types of test environment instances. For testing multiple testlets in parallel, you need to use one of Python's many parallel computing libraries, or third-party tool such as xargs or GNU parallel.</p>"},{"location":"user-guide/parallelization/#examples","title":"Examples","text":""},{"location":"user-guide/parallelization/#lxd","title":"LXD","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Parallel testing example using LXD as test environment provider.\"\"\"\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook\nfrom cleantest.data.pkg import Pip\nfrom cleantest.provider import lxd\n@lxd(\nimage=[\"ubuntu-jammy-amd64\", \"ubuntu-focal-amd64\", \"ubuntu-bionic-amd64\"],\npreserve=False,\nparallel=True,\nnum_threads=3,\n)\ndef install_tabulate():\nimport sys\ntry:\nfrom tabulate import tabulate\nprint(\"tabulate is installed.\", file=sys.stdout)\nexcept ImportError:\nprint(\"Failed to import tabulate package.\", file=sys.stderr)\nsys.exit(1)\nsys.exit(0)\ndef test_parallel_lxd(clean_slate) -&gt; None:\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(name=\"pip_injection\", packages=[Pip(packages=\"tabulate\")])\nconfig.register_hook(start_hook)\nfor name, result in install_tabulate():\ntry:\nassert result.exit_code == 0\nexcept AssertionError:\nraise Exception(f\"{name} failed. Result: {result}\")\n</code></pre>"},{"location":"user-guide/test-env-providers/","title":"Test environment providers","text":"<p>Test environment providers are the backbone of cleantest; they provide the containers or virtual machines that cleantest injects the testlets into. Test environment providers can be thought of as hypervisors that can be controlled from Python. To control test environment providers, cleantest uses Python decorators. These decorators accept arguments from the user and capture the body of the testlet.</p> <p>The following is a list of all the supported test environment providers in cleantest and how to control them.</p>"},{"location":"user-guide/test-env-providers/#lxd","title":"LXD","text":"<p>The <code>lxd</code> decorator handles running testlets inside of containers and/or virtual machines controlled by the LXD hypervisor. The decorator accepts the following arguments:</p> <ul> <li><code>name (str)</code>: Name for test environment (Default: \"test\").</li> <li><code>image (List[str])</code>: LXD image to use for test environment (Default: [\"ubuntu-jammy-amd64\"]).</li> <li><code>preserve (bool)</code>: Preserve test environment after test has completed (Default: True).</li> <li><code>parallel (bool)</code>: Run test environment instances in parallel (Default: False).</li> <li><code>num_threads (int)</code>: Number of threads to use when running test environment instances in parallel (Default: None).</li> </ul>"},{"location":"user-guide/test-env-providers/#example-usage","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of LXD test environment provider.\"\"\"\nimport os\nimport pathlib\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook\nfrom cleantest.data.pkg import Charmlib, Pip\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef install_snapd():\nimport sys\nimport charms.operator_libs_linux.v0.apt as apt\ntry:\napt.update()\napt.add_package(\"snapd\")\nprint(\"snapd installed.\", file=sys.stdout)\nexcept apt.PackageNotFoundError:\nprint(\"Package could not be found in cache or system.\", file=sys.stderr)\nsys.exit(1)\nexcept apt.PackageError as e:\nprint(f\"Could not install package. Reason: {e}\", file=sys.stderr)\nsys.exit(1)\ntry:\nsnapd = apt.DebianPackage.from_installed_package(\"snapd\")\nprint(f\"snapd version {snapd.fullversion} is installed.\", file=sys.stdout)\nexcept apt.PackageNotFoundError:\nprint(\"Snapd failed to install.\", file=sys.stderr)\nsys.exit(1)\ntry:\nfrom tabulate import tabulate\nprint(\"tabulate is installed.\", file=sys.stdout)\nexcept ImportError:\nprint(\"Failed to import tabulate package.\", file=sys.stderr)\nsys.exit(1)\nsys.exit(0)\ndef test_local_lxd(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"setup_deps\",\npackages=[\nCharmlib(\nauth_token_path=root / \"charmhub.secret\",\ncharmlibs=[\"charms.operator_libs_linux.v0.apt\"],\n),\nPip(requirements=[root / \"requirements.txt\"]),\n],\n)\nconfig.register_hook(start_hook)\nfor name, result in install_snapd():\nassert result.exit_code == 0\n</code></pre>"},{"location":"user-guide/using-diff-linux-distro/","title":"Using different Linux distributions","text":"<p>This page is currently under construction</p> <p>Hey there from NucciTheBoss! cleantest is not yet able to support multiple Linux distributions like Debian, CentOS Stream, Alma Linux, Fedora, etc. The only distro officially supported right now is Ubuntu, mostly because Ubuntu is my distro of choice, and well... I work at Canonical. You can to pull your own images with the LXD test environment provider, but I have not gotten around to making that example yet. </p> <p>Mostly what needs to be done to support multiple distros is to refactor the <code>LXDDataStore</code> class. Currently I have the defaults hardcoded in, but I would like to potentially make cleantest more intelligent. Rather than hardcode, I would like to enable querying of the <code>images:</code> endpoint and other endpoints. That information can then be loaded in at runtime so that users will always have an update-to-date image list. Please bear with me while I work  to make cleantest even better!</p> <p>Multidistro support will be added in cleantest 0.4.0</p>"},{"location":"user-guide/artifacts/directory/","title":"Working with directories","text":"<p>Directories can be uploaded to and downloaded from test environment instances as artifacts.</p>"},{"location":"user-guide/artifacts/directory/#dir-class","title":"Dir class","text":"<p>The <code>Dir</code> class represents a directory. Its constructor accepts three arguments:</p> <ul> <li><code>src (str)</code>: File path to load directory from.</li> <li><code>dest (str)</code>: File path for where to dump directory.</li> <li><code>overwrite (bool)</code>: Overwrite the directory if it already exists at dest. Defaults to False.</li> </ul> <code>Dir</code> versus <code>File</code> <p>When the <code>Dir</code> class's load method is invoked, an exception will be raised if src is determined to be a file and not a directory. This exception is raised because cleantest handles directories differently than files when dumping out to dest. If you are working with files, not directories, you should use the <code>File</code> class instead.</p>"},{"location":"user-guide/artifacts/directory/#supported-hooks","title":"Supported hooks","text":"<p>The <code>Dir</code> class's behavior changes depending on the hook it is used with. Here is a list of hooks that support <code>Dir</code> and how <code>Dir</code> behaves when accessed by them:</p> <code>StartEnvHook</code> <p>src is loaded from local system and dest is the location to dump the directory inside the test environment instance.</p> <code>StopEnvHook</code> <p>src is loaded from the test environment instance and dest is the location to dump the directory on the local system.</p>"},{"location":"user-guide/artifacts/directory/#example-usage","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of Dir class.\"\"\"\nimport os\nimport pathlib\nimport shutil\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook, StopEnvHook\nfrom cleantest.data import Dir\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef work_on_artifacts():\nimport os\nimport pathlib\nimport sys\nprint(pathlib.Path(\"/root/greetings\").is_dir(), file=sys.stdout)\nos.mkdir(\"/root/dump\")\npathlib.Path(\"/root/dump/dump_1.txt\").write_text(\"Oh I have been dumped again!\")\ndef test_upload_download(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"upload_artifact\",\nupload=[\nDir(root / \"greetings\", \"/root/greetings\"),\n],\n)\nstop_hook = StopEnvHook(\nname=\"download_artifact\",\ndownload=[\nDir(\"/root/dump\", root / \"dump\", overwrite=True,),\n],\n)\nconfig.register_hook(start_hook, stop_hook)\nfor name, result in work_on_artifacts():\nassert (root / \"dump\").is_dir() is True\nshutil.rmtree(root / \"dump\")\n</code></pre>"},{"location":"user-guide/artifacts/file/","title":"Working with files","text":"<p>Files can be uploaded to and downloaded from test environment instances as artifacts.</p>"},{"location":"user-guide/artifacts/file/#file-class","title":"File class","text":"<p>The <code>File</code> class represents a file. Its constructor accepts three arguments:</p> <ul> <li><code>src (str)</code>: File path to load file from.</li> <li><code>dest (str)</code>: File path for where to dump file.</li> <li><code>overwrite (bool)</code>: Overwrite the file if it already exists at <code>dest</code>. Defaults to False.</li> </ul> <code>File</code> versus <code>Dir</code> <p>When the <code>File</code> class's load method is invoked, an exception will be raised if src is determined to be a directory  and not a file. This exception is raised because cleantest handles files differently than directories when  dumping out to dest. If you are working with directories, not files, you should use the <code>Dir</code>  class instead.</p>"},{"location":"user-guide/artifacts/file/#supported-hooks","title":"Supported hooks","text":"<p>The <code>File</code> class's behavior changes depending on the hook it is used with. Here is a list of hooks that support <code>File</code> and how <code>File</code> behaves when accessed by them:</p> <code>StartEnvHook</code> <p>src is loaded from local system and dest is the location to dump the file inside the test environment instance.</p> <code>StopEnvHook</code> <p>src is loaded from the test environment instance and dest is the location to dump the file on the local system.</p>"},{"location":"user-guide/artifacts/file/#example-usage","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of File class.\"\"\"\nimport os\nimport pathlib\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook, StopEnvHook\nfrom cleantest.data import File\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef work_on_artifacts():\nimport pathlib\nimport sys\nprint(pathlib.Path(\"/root/greeting.txt\").is_file(), file=sys.stdout)\npathlib.Path(\"/root/dump.txt\").write_text(\"Dumped like a sack of rocks\")\ndef test_upload_download(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"upload_artifact\",\nupload=[\nFile(root / \"greeting.txt\", \"/root/greeting.txt\"),\n],\n)\nstop_hook = StopEnvHook(\nname=\"download_artifact\",\ndownload=[\nFile(\"/root/dump.txt\", root / \"dump.txt\", overwrite=True,),\n],\n)\nconfig.register_hook(start_hook, stop_hook)\nfor name, result in work_on_artifacts():\nassert (root / \"dump.txt\").is_file() is True\n(root / \"dump.txt\").unlink(missing_ok=True)\n</code></pre>"},{"location":"user-guide/packages/charmlib/","title":"Working with charm libraries","text":"<p>Charm libraries are special Python modules used in charmed operators deployed by Juju. They are an easy way to distribute reusable code without needing to involve any particular package build system. Fundamentally, charm libraries are used to provide a means for charm developers to make the implementation of any  relation they define as simple as possible for other charm developers.</p> Note <p>Comphrensive documentation on how to write/develop/use charm libraries are beyond the scope of this documentation. If you are interested in learning more about charm libraries, please refer to their official documentation here: https://juju.is/docs/sdk/libraries</p>"},{"location":"user-guide/packages/charmlib/#charmlib-class","title":"Charmlib class","text":"<p><code>Charmlib</code> is a package metaclass that represents the charm library installation command charmcraft fetch-lib. Its constructor accepts two arguments:</p> <ol> <li><code>auth_token_path (str)</code>: File path to a Charmhub authentication token. This token is needed to download charm     libraries from charmhub.io.</li> <li><code>charmlibs (List[str])</code>: List of charm libraries to install inside the test environment instance.</li> </ol> <p>Charm libraries are not installed a special directory such as site-packages or dist-packages; they are directly installed to a lib/ directory under your current working directory. Therefore, the <code>Charmlib</code> class modifies the PYTHONPATH environment variable to inform the Python interpreter that there are importable modules under lib/.</p>"},{"location":"user-guide/packages/charmlib/#example-usage","title":"Example usage","text":"<p>First, you need to create a Charmhub authentication token. This can be accomplished by using the following command in your shell:</p> <pre><code>charmcraft login --export charmhub.secret\n</code></pre> <p>After authenticating with Charmhub (you may need to create an Ubuntu One account), you can now use the example test script below:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of Charmlib package metaclass.\"\"\"\nimport os\nimport pathlib\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook\nfrom cleantest.data.pkg import Charmlib\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef install_snapd():\nimport sys\nimport charms.operator_libs_linux.v0.apt as apt\ntry:\napt.update()\napt.add_package(\"snapd\")\nprint(\"snapd installed.\", file=sys.stdout)\nexcept apt.PackageNotFoundError:\nprint(\"Package could not be found in cache or system.\", file=sys.stderr)\nsys.exit(1)\nexcept apt.PackageError as e:\nprint(f\"Could not install package. Reason: {e}\", file=sys.stderr)\nsys.exit(1)\ntry:\nsnapd = apt.DebianPackage.from_installed_package(\"snapd\")\nprint(f\"snapd version {snapd.fullversion} is installed.\", file=sys.stdout)\nexcept apt.PackageNotFoundError:\nprint(\"Snapd failed to install.\", file=sys.stderr)\nsys.exit(1)\nsys.exit(0)\ndef test_local_lxd(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"setup_deps\",\npackages=[\nCharmlib(\nauth_token_path=root / \"charmhub.secret\",\ncharmlibs=[\"charms.operator_libs_linux.v0.apt\"],\n),\n],\n)\nconfig.register_hook(start_hook)\nfor name, result in install_snapd():\nassert result.exit_code == 0\n</code></pre>"},{"location":"user-guide/packages/pip/","title":"Working with pip packages","text":"<p>Pip is the package installer for the Python programming language. You can use it to install packages from the Python Package Index (PYPI).</p>"},{"location":"user-guide/packages/pip/#pip-class","title":"Pip class","text":"<p><code>Pip</code> is a package metaclass that represents the Python package installation command pip install. Its constructor accepts three arguments:</p> <ul> <li><code>packages (List[str])</code>: List of packages to install. Packages are pulled from PYPI. Defaults to None.</li> <li><code>requirements (List[str])</code>: List of paths to requirements.txt files. Defaults to None.</li> <li><code>constraints (List[str])</code>: List of paths to constraints.txt files. Defaults to None.</li> </ul> Warning about using multiple requirements and constraints files <p>The requirements and constraints fields in <code>Pip</code> are one-to-one. If you define multiple requirements files to use and want to use a constraints file with one of the requirements files, then you will need to define a constraints file for each of the requirements files in the list. If the length of requirements files list does not match the length of the constraints files list, then an exception will be raised.</p> <p>Therefore, if you have multiple requirements files and only one uses a constraints file, it is better to use instantiate two instances of <code>Pip</code> class rather than one. One instance can be used for the requirements and  constraints file pairing while the other can be used for the rest of the requirements files.</p>"},{"location":"user-guide/packages/pip/#example-usage","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of Pip package metaclass.\"\"\"\nimport os\nimport pathlib\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook\nfrom cleantest.data.pkg import Pip\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef install_snapd():\nimport sys\ntry:\nfrom tabulate import tabulate\nprint(\"tabulate is installed.\", file=sys.stdout)\nexcept ImportError:\nprint(\"Failed to import tabulate package.\", file=sys.stderr)\nsys.exit(1)\nsys.exit(0)\ndef test_local_lxd(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"setup_deps\",\npackages=[\nPip(requirements=[root / \"requirements.txt\"]),\n],\n)\nconfig.register_hook(start_hook)\nfor name, result in install_snapd():\nassert result.exit_code == 0\n</code></pre>"},{"location":"user-guide/packages/snaps/","title":"Working with snap packages","text":"<p>Snap packages are a bundle of an app and its dependencies that work across a variety of Linux distributions without modification. They are automatically managed and maintained by the snapd  service running as a daemon in the background. Snaps can be run on servers, desktops, or internet-of-things devices;  they greatly reduce the time-to-market of deploying applications to devices running Linux.</p> Note <p>Comprehensive documentation on how to write and/or develop snap packages are beyond the scope of this documentation. If you are interested in learning more about snap packages, please refer to their official documentation here: https://snapcraft.io/docs</p>"},{"location":"user-guide/packages/snaps/#confinement-class","title":"Confinement class","text":"<p><code>Confinement</code> is an enum that represents the three possible confinement modes for a snap: strict, classic,  and devmode. Snaps are commonly run in strict confinement, but certain snaps are granted classic confinement which gives the snap unfettered access to the underlying host system. devmode is used when developing snaps to determine which interfaces need to be defined and connected.</p> <p>The class takes no arguments, but it has the following attributes:</p> <ul> <li><code>STRICT</code>: Represents strict confinement.</li> <li><code>CLASSIC</code>: Represents classic confinement.</li> <li><code>DEVMODE</code>: Represents devmode confinement.</li> </ul>"},{"location":"user-guide/packages/snaps/#plug-class","title":"Plug class","text":"<p><code>Plug</code> is a metaclass that represents a snap plug. Plugs are used to connect a snap package to another snap package. Its constructor accepts two arguments:</p> <ul> <li><code>snap (str)</code>: Name of the snap that provides the plug. Defaults to None.</li> <li><code>name (str)</code>: Name of the plug. Defaults to None.</li> </ul>"},{"location":"user-guide/packages/snaps/#slot-class","title":"Slot class","text":"<p><code>Slot</code> is a metaclass that represents a snap slot. Slots are to accept connections from other snap packages. Its constructor accepts two arguments:</p> <ul> <li><code>snap (str)</code>: Name of the snap that provides the slot.</li> <li><code>name (str)</code>: Name of the slot.</li> </ul>"},{"location":"user-guide/packages/snaps/#connection-class","title":"Connection class","text":"<p><code>Connection</code> is a metaclass that represents the snap connect command. It is used to connect plugs to slots after the snap packages have been installed. Its constructor accepts three arguments:</p> <ul> <li><code>plug (Plug)</code>: Plug to connect.</li> <li><code>slot (Slot)</code>: Slot to connect to. Defaults to None.</li> <li><code>wait (bool)</code>: Wait for snap connect operation to complete before proceeding. Defaults to True.</li> </ul> <p><code>Connection</code> provides one private method:</p> <ul> <li><code>_lint</code>: Lint inputs passed to the constructor to ensure that snap connect will be a valid operation.</li> </ul> <p><code>Connection</code> provides one public method:</p> <ul> <li><code>connect</code>: Execute snap connect operation. Even though this method is public, it should not be used when      configuring your hooks.</li> </ul>"},{"location":"user-guide/packages/snaps/#alias-class","title":"Alias class","text":"<p><code>Alias</code> is a metaclass that represents the snap alias command. It is used to create aliases after a snap package has been installed. Its constructor accepts four arguments:</p> <ul> <li><code>snap_name (str)</code>: Name of the snap that provides the app.</li> <li><code>app_name (str)</code>: Name of the app to create an alias for.</li> <li><code>alias_name (str)</code>: Name of alias to create.</li> <li><code>wait (bool)</code>: Wait for snap alias operation to complete before proceeding. Defaults to True.</li> </ul> <p><code>Alias</code> provides one private method:</p> <ul> <li><code>_lint</code>: Lint inputs passed to the constructor to ensure that snap alias will be a valid operation.</li> </ul> <p><code>Alias</code> provides one public method:</p> <ul> <li><code>alias</code>: Execute snap alias operation. Even though this method is public, it should not be used when     configuring your hooks.</li> </ul>"},{"location":"user-guide/packages/snaps/#snap-class","title":"Snap class","text":"<p><code>Snap</code> is a package metaclass that represents the snap installation command <code>snap install</code>. Its constructor accepts eight arguments:</p> <ul> <li><code>snaps (List[str])</code>: List of snaps to install inside the test environment instance. These snaps are pulled from     the public Snap Store. Defaults to None.</li> <li><code>local_snaps (List[str])</code>: List of file paths to local snap packages to be installed inside the test environment     instance. Defaults to None.</li> <li><code>confinement (Confinement)</code>: Confinement level to install snaps with. Defaults to Confinement.STRICT.</li> <li><code>channel (str)</code>: Channel to install snap from. Only valid for snaps being pulled from store. Defaults to None.</li> <li><code>cohort (str)</code>: Key of cohort that snap belongs to/should be installed with. Defaults to None.</li> <li><code>dangerous (bool)</code>: Install unsigned snaps. Only valid for local snaps. Defaults to False.</li> <li><code>connections (List[Connection])</code>: List of connections to set up after snaps have been installed. Defaults to None.</li> <li><code>aliases (List[Alias])</code>: List of aliases to create after snaps have been installed. Defaults to None.</li> </ul> <p>The <code>Snap</code> class will attempt to install snapd inside the test environment instance if the service is not detected when the class goes to install the listed snap packages.</p>"},{"location":"user-guide/packages/snaps/#example-usage","title":"Example usage","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Example usage of Snap package metaclass with related classes.\"\"\"\nimport os\nimport pathlib\nfrom cleantest.control import Configure\nfrom cleantest.control.hooks import StartEnvHook\nfrom cleantest.data.pkg import Connection, Plug, Slot, Snap\nfrom cleantest.provider import lxd\n@lxd(image=\"ubuntu-jammy-amd64\", preserve=False)\ndef functional_snaps():\nimport sys\nfrom shutil import which\nif which(\"pypi-server\") is None:\nsys.exit(1)\nelif which(\"marktext\") is None:\nsys.exit(1)\nelse:\nsys.exit(0)\ndef test_snap_package(clean_slate) -&gt; None:\nroot = pathlib.Path(os.path.dirname(os.path.realpath(__file__)))\nconfig = Configure(\"lxd\")\nstart_hook = StartEnvHook(\nname=\"test_snaps\",\npackages=[\nSnap(\nsnaps=\"pypi-server\",\nconnections=[\nConnection(\nPlug(\"pypi-server\", \"removable-media\"),\nSlot(name=\"removable-media\"),\n)\n],\n),\nSnap(\nlocal_snaps=[root / \"marktext.snap\"],\ndangerous=True,\n),\n],\n)\nconfig.register_hook(start_hook)\nfor name, result in functional_snaps():\nassert result.exit_code == 0\n</code></pre>"}]}